# -*- coding: utf-8 -*-
"""DatosMaestro_EPM_Corr.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zYTMeugmpitYD-6YdjqwsMA0pbmoNjTf
"""

!pip install tensorflow
#Prediccion de ensa a partir de deep learning para el mes de mayo
# Importar las bibliotecas necesarias
import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, InputLayer
from matplotlib import pyplot as plt
import seaborn as sns

# Montar Google Drive (asumiendo que los datos están allí)
from google.colab import drive
drive.mount('/content/drive')

# Cargar los datos
file_path = '/content/drive/MyDrive/Colab Notebooks/DIPLOMADO EPM/Git/BD_maestro.csv'  # Ajusta a la ubicación de tu archivo CSV
datos_Maestro = pd.read_csv(file_path)

# Convertir la columna 'fecha' a datetime
datos_Maestro['fecha'] = pd.to_datetime(datos_Maestro['fecha'])

datos_Maestro.describe()

# Encontrar NaNs y valores duplicados en datos_Maestro

print('hay {} valores missing o NaNs en datos_Maestro.'
      .format(datos_Maestro.isnull().values.sum()))

temp_datos = datos_Maestro.duplicated(keep='first').sum()

print('hay {} filas duplicadas en datos_Maestro revisando todas las columnas.'
      .format(temp_datos))

# Matriz de correlación y gráficas
sns.heatmap(datos_Maestro.corr(), annot=True, cmap='blue')
plt.show()

correlations = datos_Maestro.corr(method='pearson')
print(correlations['ENSA'].sort_values(ascending=False).to_string())

datos_Maestro.dtypes

#Graficos para analisis visual exploratorio
#Definir una funcion para graficar los diferentes tipos de time-series
def plot_series(df=None, column=None, series=pd.Series([]),
                label=None, ylabel=None, title=None, start=0, end=None):

    sns.set()
    fig, ax = plt.subplots(figsize=(30, 12))
    ax.set_xlabel('Time', fontsize=16)
    if column:
        ax.plot(df[column][start:end], label=label)
        ax.set_ylabel(ylabel, fontsize=16)
    if series.any():
        ax.plot(series, label=label)
        ax.set_ylabel(ylabel, fontsize=16)
    if label:
        ax.legend(fontsize=16)
    if title:
        ax.set_title(title, fontsize=24)
    ax.grid(True)
    return ax

ax = plot_series(df=datos_Maestro, column='ENSA', ylabel='ENSA(xxx)',
                 title='ENSA actual (Primeras 2 semanas - Original)', end=24*7*2)
plt.show()

#Comportamiento de ENSA a lo largo del tiempo
import pandas as pd
import matplotlib.pyplot as plt

#muestras_fechas = datos_Maestro['fecha'].sample(n=10)  # Cambia 'n' al número deseado de muestras


# Graficar
plt.figure(figsize=(10, 6))  # Tamaño del gráfico

# Graficar la línea
plt.plot(datos_Maestro['fecha'], datos_Maestro['ENSA'], marker='o', linestyle='-')

# Etiquetas y título
plt.xlabel('Fecha')
plt.ylabel('ENSA')
plt.title('Comportamiento de ENSA a lo largo del tiempo')

# Rotar las etiquetas del eje x para mejor legibilidad
plt.xticks(rotation=45)

# Mostrar todas las divisiones de fecha en el eje x
#plt.xticks(muestras_fechas)

# Mostrar la cuadrícula
plt.grid(True)

# Mostrar el gráfico
plt.tight_layout()  # Ajustar el diseño para evitar recortes
plt.show()

#Grafica de barras para compender la distribución de la variable ENSA
# Grafica el histograma del precio actual de la electricidad

ax = datos_Maestro['ENSA'].plot.hist(bins=8, alpha=0.65)

# Borrar columnas con informacion metereologica cualitativa
datos_Maestro = datos_Maestro.drop(['dummy'], axis=1)

datos_Maestro.info()

#Preparación de datos
#Escalado y estandarizacion de los datos para el entrenamiento de modelo LSTM
# Estandarización de los datos

# Seleccionar solo columnas numéricas
datos_numericos = datos_Maestro.select_dtypes(include=[np.number])

# Estandarización de los datos numéricos
scaler = StandardScaler()
datos_escalados = scaler.fit_transform(datos_numericos)
datos_escalados = pd.DataFrame(datos_escalados, columns=datos_numericos.columns)

# Añadir de nuevo las columnas no numéricas al DataFrame escalado
for col in datos_Maestro.select_dtypes(exclude=[np.number]).columns:
    datos_escalados[col] = datos_Maestro[col]

datos_numericos.info()
#df = pd.DataFrame(datos_numericos)
#datos_Maestro.to_csv('/content/drive/MyDrive/Colab Notebooks/DIPLOMADO EPM/Git/DF2.csv', index=False)
#El codigo estaba tomando los datos numerico total y no los datos escalados, esto causaba problemas en la prediccion del modelo. Se modifica para que use los datos escalados y se hace el ajuste en fecha para visualizar los datos escalados

datos_escalados_sort =  datos_escalados.sort_values(by='fecha').copy()
datos_escalados_sort = datos_escalados_sort.set_index('fecha').copy()
datos_escalados_sort

#Definiendo la red neuronal del modelo LSTM
# Preparar datos para LSTM
window_size = 24
X = []
y = []

for i in range(window_size, len(datos_escalados_sort) - 12):  # -12 para predecir el año completo de 2024
    X.append(datos_escalados_sort.iloc[i-window_size:i, :-1].values)  # Todas las variables excepto ENSA
    y.append(datos_escalados_sort.iloc[i, -1])  # La variable ENSA

X, y = np.array(X), np.array(y)

# División en entrenamiento y validación
train_size = int(len(X) * 0.8)
X_train, X_val = X[:train_size], X[train_size:]
y_train, y_val = y[:train_size], y[train_size:]

#Analisis exploratorio
#PCA
from sklearn.decomposition import PCA
X_train = np.reshape(X_train, (X_train.shape[0], -1))

pca = PCA()
X_pca = pca.fit_transform(X_train[:train_size])

num_components = len(pca.explained_variance_ratio_) #almacena el número total de componentes generados por el modelo PCA
plt.figure(figsize=(10, 6))
plt.bar(np.arange(num_components), pca.explained_variance_ratio_)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('Componente Principal')
plt.ylabel('Varianza Explicada')
plt.show()

num_components = len(pca.explained_variance_ratio_) # Número total de componentes principales generados por el modelo PCA
componentes = np.arange(1, num_components + 1) # Componentes principales de 1 a num_components

plt.figure(figsize=(10, 6))
plt.bar(componentes[:15], pca.explained_variance_ratio_[:15]) # Limita la visualización hasta el componente 15
plt.plot(componentes[:15], np.cumsum(pca.explained_variance_ratio_[:15]))
plt.xlabel('Componente Principal')
plt.ylabel('Varianza Explicada')
plt.show()

#Arquitectura LSTM

# Definir la arquitectura LSTM
model = Sequential([
    #InputLayer((window_size, datos_numericos.shape[1] - 1)),
    InputLayer((window_size, datos_escalados_sort.shape[1] - 1)),
    LSTM(64, activation='relu'),
    Dense(8, activation='tanh'),
    Dense(1, activation='linear')

# Guardar la visualización del modelo
plot_model(model1, to_file='rnn_model.png', show_shapes=True, show_layer_names=True)

# Compilar y entrenar el modelo
model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))

#Modelo para optimizar la LSTM
    
    !pip install tensorflow
import tensorflow as tf
import numpy as np
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN,LSTM, Dense, RNN, GRU
from tensorflow.keras.optimizers import Adam, SGD
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from IPython.display import SVG

!pip install keras-tuner
import keras_tuner as kt

from kerastuner.tuners import Hyperband
from kerastuner.engine.hyperparameters import HyperParameters

# Define un objeto Hyperparameters
hp = HyperParameters()

input_shape = X_train.shape[-2:]  # Toma las últimas dos dimensiones de X_train para definir la forma de entrada del modelo.

def build_lstm_model(hp):
    #n_timesteps, n_features = X_train.shape[1], X_train.shape[2] # ELEGIR LOS HIPERPARAMETROS QUE NECESITEN OPTIMIZAR
    #n_outputs = 1  # Cambia esto si el número de salidas es diferente
    #n_layers = hp.Int('n_layers', min_value=1, max_value=10)

    modelop = Sequential()
    modelop.add(InputLayer((window_size, 17))) # Cual es el tamano de los datos de entrada


    modelop.add(LSTM(
        units=hp.Int('lstm_units', min_value=1, max_value=200, step=10),  # Número de unidades en la capa LSTM, ajustable entre 32 y 128 en pasos de 32
        input_shape=input_shape,  # La forma de los datos de entrada (debe ser definida antes de llamar a esta función)
        activation=hp.Choice('l_lstm_activation', values=['tanh', 'relu', 'sigmoid']),
        return_sequences=False  # Indica que esta capa LSTM devolverá solo la última salida en la secuencia de salida
    ))

    modelop.add(Dense(
        units=hp.Int('dense_units', min_value=1, max_value=200, step=10),  # Número de neuronas en la capa densa, ajustable entre 8 y 24 en pasos de 8
        activation=hp.Choice('l_dense_activation', values=['tanh', 'relu', 'sigmoid'])
    ))

    # Agregar otra capa densa que actuará como la capa de salida del modelo
    modelop.add(Dense(1, activation='linear'))  # Capa de salida para regresión, con activación lineal


    # Compilar el modelo
    modelop.compile(
        optimizer=Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')),  # Optimizador Adam con tasa de aprendizaje ajustable
        loss='mean_squared_error',  # Función de pérdida Mean Squared Error para problemas de regresión
        metrics=['RootMeanSquaredError']  # Métricas para evaluar el modelo durante el entrenamiento
    )

    return modelop  # Retornar el modelo compilado

# Crea el sintonizador
sintonizador = Hyperband(build_lstm_model, # PUEDEN USAR HYPERBAND O RAMDON SEARCH O BAYESIAN MODEL
                        objective="val_loss",  # Cambia a "val_loss"
                        max_epochs=10,
                        factor=3,
                        hyperband_iterations=5,
                        project_name='lstm_tuning')

# Busca los mejores hiperparámetros
sintonizador.search(X_train, y_train, validation_split=0.2)

# Obtiene la mejor configuración de hiperparámetros
best_hyperparameters = sintonizador.get_best_hyperparameters(num_trials=1)[0]

# Imprimir los mejores hiperparámetros
for key, value in best_hyperparameters.values.items():
    print(f"{key}: {value}")

# Construye y compila el modelo con los mejores hiperparámetros
best_model = build_lstm_model(best_hyperparameters)
best_model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))

# Guarda la visualización del modelo
keras.utils.plot_model(best_model, to_file='best_model.png', show_shapes=True, show_layer_names=True)

#Modelo final LSTM y optimizado
#Generar predicciones para el año 2024 y simular datos futuros para extender la base a los datos disponibles

# Definir la arquitectura LSTM
model = Sequential([
    #InputLayer((window_size, datos_numericos.shape[1] - 1)),
    InputLayer((window_size, datos_escalados_sort.shape[1] - 1)),
    LSTM(131, activation='relu'),
    Dense(151, activation='tanh'),
    Dense(1, activation='linear')
])

# Guardar la visualización del modelo
plot_model(model1, to_file='rnn_model.png', show_shapes=True, show_layer_names=True)

# Compilar y entrenar el modelo
model.compile(optimizer=Adam(learning_rate=0.005343634220268847), loss='mean_squared_error',metrics=[RootMeanSquaredError()])
model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_val, y_val))

#Generacion de caracteristicas futuras
#Extensión del conjunto de datos
#Prediccion con el modelo

import numpy as np
import pandas as pd
from datetime import timedelta
import matplotlib.pyplot as plt

periodos_pronostico = 6

# Simular datos futuros para las características necesarias basándonos en las últimas observaciones
last_date = datos_Maestro['fecha'].max()
future_dates = pd.date_range(start=last_date + timedelta(days=1), periods=periodos_pronostico, freq='M')

# Asumiendo que la tendencia de las características numéricas se mantiene constante
future_data = []
for date in future_dates:
    last_row = datos_numericos.iloc[-1].copy()
    last_row.name = date
    future_data.append(last_row)

future_df = pd.DataFrame(future_data)

#Error en la union de datos originales con los datos futuros, se realiza la validación del proceso que se va realizando de forma visual

# Concatenar los datos simulados con los datos originales
#extended_data = pd.concat([datos_numericos, future_df])
# Concatenar los datos simulados con los datos originales
extended_data = pd.concat([datos_numericos, future_df])
extended_index_ultimos_6 = extended_data.tail(periodos_pronostico).index
extended_data

extended_index = extended_data.tail(periodos_pronostico).index

# Escalar los datos extendidos
#extended_scaled = scaler.transform(extended_data)
#extended_scaled = pd.DataFrame(extended_scaled, columns=datos_numericos.columns)

# Escalar los datos extendidos
extended_scaled = scaler.transform(extended_data)
extended_scaled_df = pd.DataFrame(extended_scaled, columns=extended_data.columns) #, index=extended_index)

# Preparar datos para LSTM incluyendo los futuros
X_extended = []
y_extended = []

for i in range(window_size, len(extended_scaled_df) - periodos_pronostico):  # Preparar hasta junio de 2024
    X_extended.append(extended_scaled_df.iloc[i-window_size:i, :-1].values)  # Excluir ENSA
    y_extended.append(extended_scaled_df.iloc[i, -1])  # Incluir ENSA

X_extended = np.array(X_extended)
y_extended = np.array(y_extended)

# Predicciones para los primeros 6 meses de 2024
predictions = model.predict(X_extended[-periodos_pronostico:])  # Últimos 6 registros para predicción

predictions_series = pd.Series(predictions[:, 0], name='Predictions')
predictions_series

#Se genera el dataframe para ir ajustando en cuanto a que sera la prediccion y en conjunto con los reales

df_with_predictions = pd.concat([extended_scaled_df.tail(6).reset_index(), predictions_series], axis=1)
df_with_predictions_2 = df_with_predictions.set_index('index')
df_with_predictions_3=df_with_predictions_2.drop('ENSA',axis = 1)
df_with_predictions_4 = df_with_predictions_3.rename(columns= {'Predictions':'ENSA'})
df_with_predictions_4

#Los datos se le aplican la funcion para volver los datos normales

df_index = pd.DataFrame({'fecha': extended_index},index =df_with_predictions_4.index )

# Devolver las predicciones a la escala original
original_predictions = scaler.inverse_transform(df_with_predictions_4)
original_predictions
df_pronostico = pd.DataFrame(original_predictions, columns=df_with_predictions_4.columns, index=df_with_predictions_4.index)
df_pronostico_final = pd.concat([df_pronostico,df_index],axis=1)

# Mostrar las predicciones
print("Predicciones de ENSA para los primeros 6 meses de 2024:")
print(predictions.flatten())

# Crear un DataFrame para las predicciones
predictions_df = pd.DataFrame({
    'fecha': future_dates,
    'ENSA_pred': predictions.flatten()
})
predictions_df = predictions_df.set_index('fecha')

# Filtrar los datos desde 2022 para visualizar
historical_data = datos_Maestro[(datos_Maestro['fecha'] >= '2022-01-01') & (datos_Maestro['fecha'] <= last_date)]
historical_data = historical_data[['fecha', 'ENSA']]

#Resultado de las predicciones realizadas aplicando el escalado inverso

historical_data_pronostico = df_pronostico_final[['fecha', 'ENSA']]
historical_data_pronostico

#Gradica con los resultados de preducción para el comparativo de los resultados
# Graficar los resultados
plt.figure(figsize=(15, 7))

# Graficar la serie histórica con puntos verdes y etiquetas de valores
historical = combined_data['ENSA'][:-periodos_pronostico]
plt.plot(combined_data['fecha'][:-periodos_pronostico], historical, label='Historical ENSA', color='green', marker='o')
for i, valor in enumerate(historical):
    plt.text(combined_data['fecha'].iloc[i], valor, f'{valor:.2f}', color='white', fontsize=8, ha='right', va='bottom')

# Graficar la serie pronosticada con puntos rojos y etiquetas de valores
predicted = combined_data['ENSA'].tail(periodos_pronostico)
plt.plot(combined_data['fecha'].tail(periodos_pronostico), predicted, label='Predicted ENSA', color='red', linestyle='--', marker='o')
for i, valor in enumerate(predicted):
    plt.text(combined_data['fecha'].iloc[-periodos_pronostico + i], valor, f'{valor:.2f}', color='white', fontsize=8, ha='right', va='bottom')

# Agregar línea que une los puntos verdes y rojos
plt.plot([combined_data['fecha'][:-periodos_pronostico].iloc[-1], combined_data['fecha'].tail(periodos_pronostico).iloc[0]],
         [combined_data['ENSA'][:-periodos_pronostico].iloc[-1], combined_data['ENSA'].tail(periodos_pronostico).iloc[0]],
         color='red',linestyle='--')

# Establecer el título y etiquetas de los ejes
plt.title('Historico y Prediccion ENSA desde 2022 a 2024')
plt.xlabel('Date')
plt.ylabel('ENSA')

# Rotar los títulos del eje x para que se vean todos
plt.xticks(rotation=90)

# Ajustar los ticks del eje x para mostrar más divisiones
plt.xticks(combined_data['fecha'], rotation=90)

# Mostrar la leyenda
plt.legend()

# Establecer el fondo negro y agregar cuadrícula
plt.gca().set_facecolor('white')
plt.grid(True, color='black')

# Mostrar la gráfica
plt.show()
